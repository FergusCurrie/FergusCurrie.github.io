{"posts":[{"title":"A&#x2F;B testing","text":"A/B Testing IntroA/B testing aka single blind research design is a technique to measure the effect of a treatment on a population. It is used in many fields including medicine, marketing and software development. It is a form of statistical hypothesis testing. An example in web development is changing the background color of a button for some users and measuring how that effects the number of clicks on a button. A useful organisation of A/B testing is: setup, run and evaluate. The SetupProblem StatementDefine the A/B test. This includes defining the sucess metric (fraction of button clicks) and the user journey (user is on X page, sees the button, user clicks button). The user journey is important to correctly define the population of interest. Hypothesis TestingA throw back to STAT292, the null $H_0$ and alternate hypothesis $H_A$ are defined. The null hypothesis is that the treatment has no effect on the population. The alternate hypothesis is that the treatment does have an effect on the population - for an A/B test this is generally directional, we want an increase in population mean. A significance level is the probability of rejecting the null hypothesis when it is true (type I error). This is usually set to 5% (arbitary). Another parameter is the statistical power which is the probability of correctly rejecting the null hypothesis when it is false. This is commonly set to 80%. Experiment Design Randomisation unit (population that gets split) Target populatiuon (population that is measured) Sample size $n \\approx \\dfrac{16\\sigma^2}{\\delta^2}$ Duration (1-2 weeks, capture seasonality of days of week) RunRunning requires setting up the application to stream users into different buckets. This is usually done by a random number generator. The random number generator is seeded by the user id. This ensures that the same user is always in the same bucket. Instrumentation must record user data to measure the metric defined earlier and store in a database. EvaluateValidity CheckCheck that the experiment was run correctly. This includes checking that the randomisation was correct and that the experiment was run for the correct duration. An important step of the validity check is considering what biases may have effected the experiment, here is a brief list: Bias Issue What to do Instrumentation Effect Experiment code incorrect, bad data as a result External Factors Holidays, Covid,… abnormal data Selection bias Control and Treatment systematically different A/A Test (perform test on two control groups and validate they are the same) Sample Missmatch Ratio Difference in designed split and actual split Chi-squared Goodness of fit Novelty Effect Users behaviour effected just because feature is new Segment users into old and new, review performance on new users Interpret ResultsAccept or reject the null hypothesis using a test statistic. Commonly a two sample t-test to compare the two means. Launch Considerations Risk of type I error (null hypothesis rejected despite being true) Cost of launching the feature","link":"/2023/06/26/ab_testing/"},{"title":"Coursera - TensorFlow Advanced Techniques Specialisation","text":"I just finished DeepLearning.AI’s advanced TensorFlow course through coursera. The specialisatoin is divided into four courses; Custom Models, Layers and Loss Functions with TensorFlow Custom and Distributed Training with TensorFlow Advanced Computer Vision with TensorFlow Generative Deep Learning with TensorFlow I’ll go into more detail on each course below, give a few critiques, but I’ll start with a brief overview of my experience with the course. Overall I enjoyed the course but found the second half a bit underwhelming. It brushed me up on some of the finer points of my core knowledge of TensorFlow, but didn’t really teach me anything new. I think the course isn’t as focsued as it should be, it feels like a mixutre of an introudciton to TensorFlow beyond the sequential API and an introductory deep learning course. The course can be found here: TensorFlow: Advanced Techniques Specialisation. Custom Models, Layers and Loss Functions with TensorFlowThis was probably my favourite course in the specialization. It was closer to what I was looking for in terms of the inner working of TensorFlow rather than the applications. The course covered the following topics; Functional vs Sequential API, Custom losses, layers, models and callbacks. Custom and Distributed Training with TensorFlowMost of the distributed training felt supuflous, although some familiaryt with the basic TensorFLow objects for this is useful. The course covered the following topics; Differentiation and gradients with gradient tape, custom training loops, graph mode vs eagerm ode and dsitributed training strategies. Advanced Computer Vision with TensorFlowI didn’t love this course. It was a bit of a whirl-wind tour of using the techniques from course 1 and course 2 to build a few different models accross problems including segmentation, classification and object detection. It was a bit too applied for my taste, I was familiar with most of the concepts and if i’m going to spend time training models I would prefer to do it on my own projects. Generative Deep Learning with TensorFlowI actually quite enjoyed this course despite it’s similarities to course 3. It’s pretty hard not to appreciate the results of the generative techniques we covered. It was broken down into four chunks: neural style transfer, which I had read the paper for but not implemented, autoencoders, variational autoencoders and GANs. Critiques of the SpecializationI have three main criques on the course which I’ll go into in more detail below. Topic choice andgamification of coursera. My problem with the content, primarily in the latter courses, is that it wasn’t very specific to TensorFlow. It was more of a general introduction to deep learning and the techniques used in the field - with implementations in TensorFlow. I think for introductory courses this makes sense, trying to get the earner interested in the field and giving motivating examples. However, I think for a specialization it should be more specific to the framework, really digging into how things work. This would serve me better as a developer. The primary issue i’ve faced with all coursera work is the natural gamification of the platform. The videos, quizes and assignments, quizes are all relatively bite-sized and easy to consume. This is great for getting it done quickly, but it’s not great for retaining information. I’ve found it difficult not to use just get the videos out of the way and smash out the assignment as quickly as possible. I think this is a problem with the platform, not the course. It’s too tempting to just get the work done and move on. I think a more effective approach would be to have longer videos in a more lecture style format, with more content. THe assignments should be removed and replaced with longer quizes, including random sampling of questions to prevent brute forcing, limited retries, and short answer coding questions to really test understanding.This format would create much more pressure on the learner, which is bound to give better results.","link":"/2023/03/29/advanced_tf/"},{"title":"Cutting Plane Algorithms","text":"Cutting plane methods are a useful tool for solving optimization problems. In particular they solve non-differentiable convex problems. The general problem they solve is finding any point in a set $X \\in \\mathbb{R}^n $.The cutting-plane method is a generalization of bisectoin in $\\mathbb{R}$. The simplest example is the following: Set constraints: $P_k = []$ Pick a point $x^{(k)} \\in P_k$ If $x^{(k)} \\in X$ then stop. Calculate a subgradient $g$ at $x$ Add the inequality $g^{T} z \\leq x^{(k)}$ to the set of constraints. goto 2. There are a few options for picking $x^{(k)} \\in P_k$. The simplest is to pick a random point in the polyhedron. The others try to pick $x^{(k)}$ in an intelligent way, such that a cut will reduce the search space as much as possible. Four methods are: Center of gravity cutting-plane method Chebyshev center cutting-plane method Maximum volume ellpsoid method Analytic Cutting plane Method Ellipsoid method The extensions all seek to find a better point $x^{(k)}$ than a random selection. We basically want the cutting plane to reduce the search space as much as possible. Therefore the center of the polyhedron is a good choice. Center of Gravity Cutting-plane MethodThis algorithm is so inefficient that it’s considered a theoretical approach. It does highlight exactly what these extensions are trying to do. It picks the next search point as the center of gravity of the polyhedron. $$x^{(k+1)} = \\dfrac{\\int_{P_k}xdx}{\\int_{P_k}dx}$$ Chebyshev Center Cutting-plane MethodThis method picks the next search point as the center of the largest ball that fits inside the polyhedron. This can be founded via an LP. It a common geometric optimisation problem called Chebshev Centering. $$\\begin{aligned}\\max \\quad &amp; R \\\\textrm{s.t.} \\quad &amp; a_i^T x + R ||a_i||_* \\leq b_i, i = 1,…,m \\\\quad &amp; R \\geq 0 \\\\end{aligned}$$ Maximum volume ellipsoid methodThis is similar to the chebyshev method in that it finds the largest ellipsoid which fits in the polyhedra and then chooses the center of that ellipsoid as the next search point. Find the ellipsoid is another example of a geometric optimisation problem. It can be solved using the following LP: $$\\begin{aligned}\\max \\quad &amp; \\log \\det B^{-1} \\\\textrm{s.t.} \\quad &amp; ||Ba_i||_2 + a_i^Td \\leq b_i, i = 1,…,m \\\\end{aligned}$$ The set $[Bu + d | ||u||2 \\leq 1]$ parameterises an ellpisoid. The volume is proportional to the determinant of $B$. Therefore the volume is maximized by minimizing the determinant of $\\log \\det B^{-1}=- \\log \\det B $. 1234567891011121314151617181920212223def maximum_volume_ellipsoid(a, f): &quot;&quot;&quot;a,f define polyhedron: {z | a.T @ z &lt;= f} Given these parameters, find the largest contained polyhedron. (size proportional to det B) Args: a (list): list of normal vectors f (list): list of values &quot;&quot;&quot; a = np.array(a) n = len(a[0]) B = cp.Variable((n,n), PSD=True) d = cp.Variable(n) contraints = [ B @ d ] for i in range(len(a)): contraints.append(cp.norm(B @ a[i], 2) + a[i].T @ d &lt;= f[i]) obj = cp.Minimize(-cp.log_det(B)) problem = cp.Problem(obj, contraints) return problem.solve(), B.value, d.value Here’s a toy example of the optimisation at work: Analytic Cutting Plane MethodThe basic idea here is to find the ‘analytic center’ of the polyhedron. This optimises a log barrier funciton which can be computed using the infeasible start newton method. Ellipsoid MethodThis method is a bit different to the others. It maintains an ellipsoid which represents the current search space. It then finds a cutting plane which cuts the ellipsoid in half. It then shrinks the ellipsoid to minimially cover the polyhedron. This is repeated until the ellipsoid is small enough.","link":"/2023/04/02/cuttingplane/"},{"title":"Fourier Transform","text":"Fourier transform is one of the most important equations in applied maths. For audio deep learning this transform is used to find the spectrogram - which is a key input to the model. This blog post is a review of the fourier transform. 123import cvxpyimport matplotlib.pyplot as pltimport librosa Begin by loading the wave form and plotting 123456789audio_path = &quot;speech.wav&quot;waveform, sample_rate = librosa.load(audio_path, sr=None) # sr=None preserves sample rateplt.figure(figsize=(14, 5))plt.plot(waveform)plt.title(f&quot;Waveform. sample rate={sample_rate}Hz&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.show() Now lets look at what’s going on zoomed in 12345678910def s(t): return waveform[1000:2000][t]t = np.arange(1000)plt.figure(figsize=(14, 5))plt.plot(s(t))plt.title(f&quot;Waveform. sample rate={sample_rate}Hz&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.show() Okay now a brief review of sinusoids 12345678910111213141516import numpy as npdef sinusoid(t,freq,phase): return np.sin(2 * np.pi * (freq * t - phase))t = np.linspace(0,10,1000)plt.figure(figsize=(14, 5))for i in range(3): y = sinusoid(t, 1, i*0.1) plt.plot(t, y, label=f'phase={i*0.1}')plt.title(f&quot;Sinusoid phase example&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() The phase is a horizontal shift of the sin wave. 12345678910111213141516import numpy as npdef sinusoid(t,freq,phase): return np.sin(2 * np.pi * (freq * t - phase))t = np.linspace(0,10,1000)plt.figure(figsize=(14, 5))for i in range(1,4): y = sinusoid(t, 0.5*i, 1) plt.plot(t, y, label=f'freq={i*0.5}')plt.title(f&quot;Sinusoid frequency example&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() The fourier transform converts from the time domain to the frequency domain. It does this by representing the signal as infinitely many sinusoids of varying frequency. The main steps are: Enumerate accross frequency For each frequency optimise phase to maximise similarity of sinusoid with signal Calculate magnitude as similarity. 12345678910111213freq = 0.013phase = 1plt.figure(figsize=(14, 5))t = np.arange(1000)y = sinusoid(t, freq, phase) * .1plt.plot(s(t), label='origianl signal')plt.plot(t, y, label=f'sinusoid')plt.title(f&quot;Maximising similarity for freq={freq} sinusoid by shifting phase={phase}&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() They’re completly out of phase here. But we can tweak the phase to try have peaks matching. 12345678910111213freq = 0.013phase = 0.5plt.figure(figsize=(14, 5))t = np.arange(1000)y = sinusoid(t, freq, phase) * .1plt.plot(s(t), label='origianl signal')plt.plot(t, y, label=f'sinusoid')plt.title(f&quot;Maximising similarity for freq={freq} sinusoid by shifting phase={phase}&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() That’s a bit better. To measure the similarity between the signal and the sinusoid specified by frequency and phase we use an inner product. $$ \\phi_f = argmax_{\\phi \\in [0,1]} \\left[ \\int s(t) \\sin(2 \\pi (ft - \\phi)) dt\\right] $$ So the similarity is the area of the product of the original signal and the sinusoid. Intuitievly if they’re out of phase they cancel each other and the area is minimised. The magnitude (similarity) is then:$$ \\phi_f = max_{\\phi \\in [0,1]} \\left[ \\int s(t) \\sin(2 \\pi (ft - \\phi)) dt \\right] $$ Looking at the out of phase product: 12345678910111213freq = 0.013phase = 1plt.figure(figsize=(14, 5))t = np.arange(1000)y = sinusoid(t, freq, phase) * .1plt.plot(t, s(t) * y, label=f'product')plt.plot(np.zeros(1000))plt.title(f&quot;Out of phase Product between signal and sinusoid for freq={freq} by shifting phase={phase}&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() Notice it’s mostly negative And now looking at the inphase product: 12345678910111213freq = 0.013phase = 0.5plt.figure(figsize=(14, 5))t = np.arange(1000)y = sinusoid(t, freq, phase) * .1plt.plot(t, s(t) * y, label=f'product')plt.plot(np.zeros(1000))plt.title(f&quot;Product between signal and sinusoid for freq={freq} by shifting phase={phase}&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() Mostly positive. This in-phase product will have a much higher area, which will be calculated with the integral. So a simple fourier transform would iterative over a bunch of frequencies, for each find phase that maximises inner product with signal, and record the magnitude (area/similarity). Will do that now: 123456789101112131415num_phases = 10num_freqs = 1000freq_max_magnitudes = np.zeros(num_freqs)for freq_i, freq in enumerate(np.linspace(0, 0.1, num_freqs)): phase_magnitudes = np.zeros(num_phases) for phase_i, phase in enumerate(np.linspace(0, 1, num_phases)): magnitude = 0 for t in np.arange(1000): prod = s(t) * np.sin(np.pi * 2 * (freq * t - phase)) magnitude += prod phase_magnitudes[phase_i] = magnitude freq_max_magnitudes[freq_i] = np.max(phase_magnitudes) 12345678plt.figure(figsize=(14, 5))f = np.linspace(0, 0.1, num_freqs)plt.plot(f, freq_max_magnitudes)plt.title(f&quot;Spectrograph&quot;)plt.xlabel(&quot;Frequency&quot;)plt.ylabel(&quot;Magnitude&quot;)plt.legend()plt.show() This concludes the first section on the basics of the fourier transform. I will now move on to representing this in a cleaner way, using complex numbers. Complex Number Fourier TransformThe key idea is encoding the magnitude and phase as a complex number. The magnitude is the length of the complex number, and the phase is the angle. “The complex fourier transform coefficient”. $$\\hat{g}(f) = c_f = \\dfrac{d_f}{\\sqrt{2}} e^{-i2\\pi \\phi f}$$ $$\\hat{g}(f) = \\int g(t) e^{-i 2 \\pi f t}dt$$Where $c_f$ is the complex fourier transform coefficient. THe fourier transform is the gunction $\\hat{g} : R \\rightarrow C$, a function mapping forom real numbers (frequency) to complex numbers (complex fourier transform coefficient). 12345678910import matplotlib.pyplot as pltimport numpy as npfrom PIL import Imageimport imageiofrom IPython.display import Image, display# Generate your matplotlib plots# Replace the code below with your own plot generation logic First a remineder on complex numbers . $ i = \\sqrt{ -1 }$ Usually expressed with real and complext part z = 3 + 2j Next lets look at the euler formula 1234567891011theta = np.linspace(0, 2*np.pi, 50)euler_form = np.exp(z * theta)x = euler_form.realy = euler_form.imag# Create a scatter plotplt.scatter(x, y, color='red')plt.xlabel('Real')plt.ylabel('Imaginary')plt.title('Complex Number') We can see it traces the unit sphere. It might be helpful to look at a GIF of this in action 12345678910111213141516171819202122232425262728293031323334import osnum_frames = 40frames = []if os.path.exists('frames'): os.system('rm -rf frames')os.mkdir('frames')for i in range(num_frames): theta_ = theta[:i] euler_form = np.exp(z * theta_) x = euler_form.real y = euler_form.imag plt.plot(x, y) plt.title(f&quot;Frame {i+1}&quot;) plt.xlabel(&quot;Real&quot;) plt.ylabel(&quot;Imaginary&quot;) plt.xlim(-1.1, 1.1) plt.ylim(-1.1, 1.1) plt.gca().set_aspect('equal') # Save the current plot as an image plt.savefig(f&quot;frames/frame_{i}.png&quot;) plt.close() # Close the figure to avoid memory leaks # Read the saved image and append it to the frames list frames.append(imageio.imread(f&quot;frames/frame_{i}.png&quot;))if os.path.exists('frames'): os.system('rm -rf frames')# Save the frames as a GIFimageio.mimsave(&quot;euler.gif&quot;, frames, duration=0.1) This shows how the euler formula traces the unit sphere on the complex plane counter clockwise. This gives an easy way of representing the polar coordinates of a complex number c: $$ c = |c|\\exp{(i \\theta)} $$ where $\\exp{(i \\theta)}$ gives a direction and the absolute length of c gives the distance to the complex number c. The point of all this complex stuff is to use the magnitude and phase as polar coordinates. Phase gives the roation while magnitude gives the distance. We can then write the fourier coefficient as : $$ c_f = \\dfrac{d_f}{\\sqrt 2 } \\exp(-i2 \\pi\\theta f)$$ The minus sign in the exponent switches to tracking clockwise. The fourier transform can then be expressed…. Inverse Fourier TransformThe inverse fourier transform is the function $\\hat{g}^{-1} : C \\rightarrow R$, a function mapping from complex numbers (complex fourier transform coefficient) to real numbers (frequency). It just means we can take the fourier transform and get back the original signal. Discrete Fourier TransformThe previous definitions have been for continuous functions. In practice everything is discrete. In the DFT the continuous signal $s(t)$ is replaced with a sequence of $n$ signal samples $x(n)$. $$ \\hat{g}(\\dfrac{k}{N}) = \\sum_n x(n) e^{-i 2 \\pi \\dfrac{k}{N} n} $$ Notice that the frequency must still be made discrete. Once again we sample and only calculate transforms for finite number (M) of frequencies. Generally set M (number frequencies) = N (number signal samples) / 2. This is the nyquist frequency. Frequency = $\\dfrac{k}{N}$. DFT is $O(N^2)$, can use the FFT to make it $O(N \\log_2 N)$. It exploits redundency accross sinusoids. The FFT is just an algorithm for computing the DFT. Short Time Fourier TransformThe fourier transform extracts frequency components from a possibly non-periodic signal. This gives us the ‘what’ of the signal but not the ‘where’. The STFT gives us the ‘where’ of the signal. It is a fourier transform applied to a windowed signal. The window is a function that is 1 for a certain time and 0 everywhere else. The window is then slid across the signal and the fourier transform is applied to each window. This gives us the frequency components of the signal at each time step.A window function looks like this: $$x_w(k) = x(k)w(k)$$ where $x(k)$ is the signal and $w(k)$ is the window function. The window function is 1 for a certain time and 0 everywhere else. Window size = length of window in # samplesFrame size = number of samples in window in # samplesHop size = number of samples to slide to right Frame size can be larger sometimes. Generally they are equal. Example of windowed signal. From DFT to STFT$$ S(m,k) = \\sum_n x(n +mH)w(n) e^{-i 2 \\pi \\dfrac{k}{N} n}$$ where $w(n)$ is the window function, $m$ is the mth window/frame number, $k$ is the frequency, $H$ is the hop size, $N$ is the frame size.The DFToutputs a spectral vector with a bin for each frequncy (N complex fourier coefficients). The STFT outputs a spectral matrix (# freqeuncy bins, # num frames). The STFT is a time-frequency representation of the signal. Finally calculating the STFT in python: 123456789&quot;&quot;&quot;librosa stftn_fft = length of the windowed signalhop_length = samples to move for each dft. win_length // 4 defaultwin_length = Each frame of audio is windowed by window of length win_length and then padded with zeros to match n_fft&quot;&quot;&quot;S = librosa.stft(s(t), n_fft=100)spectrogram = np.absolute(S) ** 2plt.imshow(spectrogram) This looks a little odd due to the size of the signal. Notice the conversion from complex to real.The spectrogram is fundemental to audio deep learning which is why I have gone through this post to get a deep understanding for the data.","link":"/2023/05/15/fourier_transform/"},{"title":"Deep Cluster","text":"IntroDue to the difficult around obtaining good labelled data, self-supervised learning has been increasing in popularity over the last five year. Yann LeCun is a major proponente and his team at Meta AI are constantly publishing papers on the topic. Their recent “Cook Book of Self-Supervised Learning” [arXiv:2304.12210] divides the field into three approaches: self-distillation, constrastive and canonical correlation analysis. Of the three I think that self-distillation is the most surprising and challenges some fundemental intuitions about what works when training these models. They are based on the experimental observation that a linear classifier trainined on the features extracted from a random initalised AlexNet encoder obtains 14% top-1 accuracy on ImageNet, where random guessing is 0.1%. In this blog post I look at one of the early approaches to self-distillation - ‘Deep Cluster’. DeepClusterSelf-distillation models use their own output to generate labels which the model will be retrained on, then the process repeats. Deep Cluster uses an AlexNet encoder as a feature extractor, and applies kmeans to the extracted features to obtain labels. Collapse is a key issue in self-supervised learning. Collapse occurs when the network learns a trival representation to solve the learning task. For example Deep Cluster could collapse by the network learning to output a single embeddding, regardless of input. This would result in each instance being grouped into the same cluster, and then the network can achieve perfect accuracy by predicting that cluster. However a constant embedding isn’t very useful, so the authors have found two mechanisms to prevent collapse. The first mechanism to prevent collapse is to encourage equiparition. Equipartion means that the same number of isntances are grouped into each cluster. To implement this; any cluster that has no instances in it will update it’s centroid to a random other centroid with a small peturbation. This causes it to take about half the other clusters instances. The second mechanism to prevent collapse is to weight the loss contribution of an instance inversely to the size of the cluster. This means large clusters will have less importance per instance, whereas small clusters will have more important instances. Implementation notesI use the FAISS implementation of PCA and kmeans. PCA is applied to the embeddings to reduce them from 9216 to 256. This makes the code more memory efficient. The paper uses whitening but this is too expensive for me to use. I substract the mean, divide by the variance and divide by the L2 norm. Clustering is really slow, it requires a full pass on the dataset.","link":"/2023/05/02/deep_cluster/"},{"title":"","text":"Intro to recommender systems","link":"/2023/06/16/intro_rec_sys/"},{"title":"LLM Tricks","text":"Majority of LLM are decoder only with a few tricks for: Data Tricks Architechture Tricks Optimisation tricks Engineering tricks Architechture Tricks LayerNorm after embedding layer ALiBI posiition embedding RoPe position embedding Lazy softmax [ref: self-attention does not need O(n^2 memory)] efficient causal multihead attention (calculate (1/2)n^2 dot products) prenorm residual connections swiGLU Optimisation Tricks AdamW Cosine schedule weight decay gradient clipping dropout optmising backprop to tradeoff between memory and speed. (save more than normal for expensive operations like Wx) Engineering tricksMixed precision trainingUse low precision where possible.fp32, great for accuracy, demanding memory. good for softmaxfp16, less accurate, fast less accuratebfp16 Efficinet cuda ‘stuff’FlashAttention","link":"/2023/06/02/llm_tricks/"}],"tags":[{"name":"statistics, data-science","slug":"statistics-data-science","link":"/tags/statistics-data-science/"},{"name":"tensorflow","slug":"tensorflow","link":"/tags/tensorflow/"},{"name":"deep-learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"optimisation","slug":"optimisation","link":"/tags/optimisation/"},{"name":"signal-processing","slug":"signal-processing","link":"/tags/signal-processing/"},{"name":"computer-vision","slug":"computer-vision","link":"/tags/computer-vision/"},{"name":"self-supervised-learning","slug":"self-supervised-learning","link":"/tags/self-supervised-learning/"},{"name":"llm","slug":"llm","link":"/tags/llm/"}],"categories":[],"pages":[{"title":"about","text":"I’m a Machine Learning Engineer from New Zealand with a computer science background focused on Artificial Intelligence and Machine Learning at Victoria University of Wellington. Previously, I worked at an AI startup as a data scientist and machine learning engineer, utilizing deep computer vision to solve remote sensing segmentation problems.","link":"/About/index.html"}]}