{"posts":[{"title":"Cutting Plane Algorithms","text":"Cutting-plane methods.Cutting plane methods are a useful tool for solving optimization problems. In particular they solve non-differentiable convex problems. The general problem they solve is finding any point in a set $X \\in \\mathbb{R}^n $.The cutting-plane method is essentially a generilisatoin of bisectoin in $\\mathbb{R}$. The simplest example is the following: Constraints = $P_k ={}$ Pick a point $x^{(k)} \\in \\mathbb{R}^n, x^{(k)} \\in P_k$ If $x^{(k)} \\in X$ then stop. Calculate a subgradient $g$ at $x$ Add the inequality $g^{(k)T} z \\leq x^{(k)})$ to the set of constraints. goto 2. ‘Exentsion’ impolies there’s a base method. BUt there’s no base method. These are just method of choosing the next point. Possible i would consider the ellipsoid method an extension.Four extensions of this method are: Center of gravity cutting-plane method Chebyshev center cutting-plane method Maximum volume ellpsoid method Analytic Cutting plane Method Ellipsoid method The extensions all seek to find a better point $x^{(k)}$ than a random selection. We basically want the cutting plane to reduce the search space as much as possible. Therefore the center of the polyhedron is a good choice. Center of Gravity Cutting-plane MethodThis algorithm is so ineffient that it’s considered a theoretical approach. It does highlight exactly what these extensions are trying to do. It picks the next search point as the center of gravity of the polyhedron. $$ x ^ {(k+1)} = \\dfrac{\\int_{P_k}xdx}{\\int_{P_k}dx} $$ Chebyshev Center Cutting-plane MethodThis method picks the next search point as the center of the largest ball that fits inside the polyhedron. This can be founded via an LP. It a common problem called Chebshev centeing. $$\\begin{aligned}\\max \\quad &amp; R \\\\textrm{s.t.} \\quad &amp; a_i^T x + R ||a_i||_* \\leq b_i, i = 1,…,m \\\\quad &amp; R \\geq 0 \\\\end{aligned}$$ Maximum volume ellipsoid methodThis is similar to the chebyshev method in that it finds the largest ellipsoid which fits in the polyhedra and then takes the center point of that Analytic Cutting Plane MethodThe basic idea here is to find the ‘analytic center’ of the polyhedron. This optimises a log barrier funciton which can be computed using the infeasible start newton method. Ellipsoid MethodIntersting subproblems: Volume of an ellipsoid Minimum cover ellipsoid of polyhedron Center of gravity Maximum volume ellipsoid inside polyhedron Largest ball in polyhedra Questions?Center of ellipsoid? Well center of ball is","link":"/2023/04/02/cuttingplane/"},{"title":"Deep Cluster","text":"Deep ClusterIntroDue to the difficult around obtaining good labelled data, self-supervised learning has been increasing in popularity over the last five year. Yann LeCun is a major proponente and his team at Meta AI are constantly publishing papers on the topic. Their recent “Cook Book of Self-Supervised Learning” [arXiv:2304.12210] divides the field into three approaches: self-distillation, constrastive and canonical correlation analysis. Of the three I think that self-distillation is the most surprising and challenges some fundemental intuitions about what works when training these models. They are based on the experimental observation that a linear classifier trainined on the features extracted from a random initalised AlexNet encoder obtains 14% top-1 accuracy on ImageNet, where random guessing is 0.1%. In this blog post I look at one of the early approaches to self-distillation - ‘Deep Cluster’. DeepClusterSelf-distillation models use their own output to generate labels which the model will be retrained on, then the process repeats. Deep Cluster uses an AlexNet encoder as a feature extractor, and applies kmeans to the extracted features to obtain labels. Collapse is a key issue in self-supervised learning. Collapse occurs when the network learns a trival representation to solve the learning task. For example Deep Cluster could collapse by the network learning to output a single embeddding, regardless of input. This would result in each instance being grouped into the same cluster, and then the network can achieve perfect accuracy by predicting that cluster. However a constant embedding isn’t very useful, so the authors have found two mechanisms to prevent collapse. The first mechanism to prevent collapse is to encourage equiparition. Equipartion means that the same number of isntances are grouped into each cluster. To implement this; any cluster that has no instances in it will update it’s centroid to a random other centroid with a small peturbation. This causes it to take about half the other clusters instances. The second mechanism to prevent collapse is to weight the loss contribution of an instance inversely to the size of the cluster. This means large clusters will have less importance per instance, whereas small clusters will have more important instances. Implementation notesI use the FAISS implementation of PCA and kmeans. PCA is applied to the embeddings to reduce them from 9216 to 256. This makes the code more memory efficient. The paper uses whitening but this is too expensive for me to use. I substract the mean, divide by the variance and divide by the L2 norm. Clustering is really slow, it requires a full pass on the dataset.","link":"/2023/05/02/deep_cluster/"},{"title":"Fourier Transform","text":"Fourier Transformtodo: nquist frequency - redundecny in dftr fourier series fourier transform fourier coefficient (complex val with mag and phase) = values for a speicifc frequency wavelets dft n2 fft nlogn (metaphorof nlogn) 123import cvxpyimport matplotlib.pyplot as pltimport librosa Begin by loading the wave form and plotting 123456789audio_path = &quot;speech.wav&quot;waveform, sample_rate = librosa.load(audio_path, sr=None) # sr=None preserves sample rateplt.figure(figsize=(14, 5))plt.plot(waveform)plt.title(f&quot;Waveform. sample rate={sample_rate}Hz&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.show() Now lets look at what’s going on zoomed in 12345678910def s(t): return waveform[1000:2000][t]t = np.arange(1000)plt.figure(figsize=(14, 5))plt.plot(s(t))plt.title(f&quot;Waveform. sample rate={sample_rate}Hz&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.show() Okay now a brief review of sinusoids 12345678910111213141516import numpy as npdef sinusoid(t,freq,phase): return np.sin(2 * np.pi * (freq * t - phase))t = np.linspace(0,10,1000)plt.figure(figsize=(14, 5))for i in range(3): y = sinusoid(t, 1, i*0.1) plt.plot(t, y, label=f'phase={i*0.1}')plt.title(f&quot;Sinusoid phase example&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() The phase is a horizontal shift of the sin wave. 12345678910111213141516import numpy as npdef sinusoid(t,freq,phase): return np.sin(2 * np.pi * (freq * t - phase))t = np.linspace(0,10,1000)plt.figure(figsize=(14, 5))for i in range(1,4): y = sinusoid(t, 0.5*i, 1) plt.plot(t, y, label=f'freq={i*0.5}')plt.title(f&quot;Sinusoid frequency example&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() The fourier transform converts from the time domain to the frequency domain. It does this by representing the signal as infinitely many sinusoids of varying frequency. The main steps are: Enumerate accross frequency For each frequency optimise phase to maximise similarity of sinusoid with signal Calculate magnitude as similarity. 12345678910111213freq = 0.013phase = 1plt.figure(figsize=(14, 5))t = np.arange(1000)y = sinusoid(t, freq, phase) * .1plt.plot(s(t), label='origianl signal')plt.plot(t, y, label=f'sinusoid')plt.title(f&quot;Maximising similarity for freq={freq} sinusoid by shifting phase={phase}&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() They’re completly out of phase here. But we can tweak the phase to try have peaks matching. 12345678910111213freq = 0.013phase = 0.5plt.figure(figsize=(14, 5))t = np.arange(1000)y = sinusoid(t, freq, phase) * .1plt.plot(s(t), label='origianl signal')plt.plot(t, y, label=f'sinusoid')plt.title(f&quot;Maximising similarity for freq={freq} sinusoid by shifting phase={phase}&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() That’s a bit better. To measure the similarity between the signal and the sinusoid specified by frequency and phase we use an inner product. $$ \\phif = argmax{\\phi \\in [0,1]} \\left[ \\int s(t) \\sin(2 \\pi (ft - \\phi)) dt\\right] $$ So the similarity is the area of the product of the original signal and the sinusoid. Intuitievly if they’re out of phase they cancel each other and the area is minimised. The magnitude (similarity) is then:$$ \\phif = max{\\phi \\in [0,1]} \\left[ \\int s(t) \\sin(2 \\pi (ft - \\phi)) dt \\right] $$ Looking at the out of phase product: 12345678910111213freq = 0.013phase = 1plt.figure(figsize=(14, 5))t = np.arange(1000)y = sinusoid(t, freq, phase) * .1plt.plot(t, s(t) * y, label=f'product')plt.plot(np.zeros(1000))plt.title(f&quot;Out of phase Product between signal and sinusoid for freq={freq} by shifting phase={phase}&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() Notice it’s mostly negative And now looking at the inphase product: 12345678910111213freq = 0.013phase = 0.5plt.figure(figsize=(14, 5))t = np.arange(1000)y = sinusoid(t, freq, phase) * .1plt.plot(t, s(t) * y, label=f'product')plt.plot(np.zeros(1000))plt.title(f&quot;Product between signal and sinusoid for freq={freq} by shifting phase={phase}&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() Mostly positive. This in-phase product will have a much higher area, which will be calculated with the integral. So a simple fourier transform would iterative over a bunch of frequencies, for each find phase that maximises inner product with signal, and record the magnitude (area/similarity). Will do that now: 123456789101112131415num_phases = 10num_freqs = 1000freq_max_magnitudes = np.zeros(num_freqs)for freq_i, freq in enumerate(np.linspace(0, 0.1, num_freqs)): phase_magnitudes = np.zeros(num_phases) for phase_i, phase in enumerate(np.linspace(0, 1, num_phases)): magnitude = 0 for t in np.arange(1000): prod = s(t) * np.sin(np.pi * 2 * (freq * t - phase)) magnitude += prod phase_magnitudes[phase_i] = magnitude freq_max_magnitudes[freq_i] = np.max(phase_magnitudes) 12345678plt.figure(figsize=(14, 5))f = np.linspace(0, 0.1, num_freqs)plt.plot(f, freq_max_magnitudes)plt.title(f&quot;Spectrograph&quot;)plt.xlabel(&quot;Frequency&quot;)plt.ylabel(&quot;Magnitude&quot;)plt.legend()plt.show() This concludes the first section on the basics of the fourier transform. I will now move on to representing this in a cleaner way, using complex numbers. Complex Number Fourier Transform12345678910import matplotlib.pyplot as pltimport numpy as npfrom PIL import Imageimport imageiofrom IPython.display import Image, display# Generate your matplotlib plots# Replace the code below with your own plot generation logic First a remineder on complex numbers . $ i = \\sqrt{ -1 }$ Usually expressed with real and complext part z = 3 + 2j 123456789101112131415# Complex numberz = 0 + 2j# Extract real and imaginary partsx = z.realy = z.imag# Create a scatter plotplt.scatter(x, y, color='red')plt.xlabel('Real')plt.ylabel('Imaginary')plt.title('Complex Number')# Show the plotplt.show() This is graphed on the imaginary/real plane. Next lets look at the euler formula 1234567891011theta = np.linspace(0, 2*np.pi, 50)euler_form = np.exp(z * theta)x = euler_form.realy = euler_form.imag# Create a scatter plotplt.scatter(x, y, color='red')plt.xlabel('Real')plt.ylabel('Imaginary')plt.title('Complex Number') We can see it traces the unit sphere. It might be helpful to look at a GIF of this in action 12345678910111213141516171819202122232425262728293031323334import osnum_frames = 40frames = []if os.path.exists('frames'): os.system('rm -rf frames')os.mkdir('frames')for i in range(num_frames): theta_ = theta[:i] euler_form = np.exp(z * theta_) x = euler_form.real y = euler_form.imag plt.plot(x, y) plt.title(f&quot;Frame {i+1}&quot;) plt.xlabel(&quot;Real&quot;) plt.ylabel(&quot;Imaginary&quot;) plt.xlim(-1.1, 1.1) plt.ylim(-1.1, 1.1) plt.gca().set_aspect('equal') # Save the current plot as an image plt.savefig(f&quot;frames/frame_{i}.png&quot;) plt.close() # Close the figure to avoid memory leaks # Read the saved image and append it to the frames list frames.append(imageio.imread(f&quot;frames/frame_{i}.png&quot;))if os.path.exists('frames'): os.system('rm -rf frames')# Save the frames as a GIFimageio.mimsave(&quot;euler.gif&quot;, frames, duration=0.1) This shows how the euler formula traces the unit sphere on the complex plane counter clockwise. This gives an easy way of representing the polar coordinates of a complex number c: $$ c = |c|\\exp{(i \\theta)} $$ where $\\exp{(i \\theta)}$ gives a direction and the absolute length of c gives the distance to the complex number c. The point of all this complex stuff is to use the magnitude and phase as polar coordinates. Phase gives the roation while magnitude gives the distance. We can then write the fourier coefficient as : $$ c_f = \\dfrac{d_f}{\\sqrt 2 } \\exp(-i2 \\pi\\theta f)$$ The minus sign in the exponent switches to tracking clockwise. The fourier transform can then be expressed…. Inverse Fourier Transform… Discrete Fourier Transform… Fast Fourier Transform… Short Time Fourier Transform…","link":"/2023/05/15/fourier_transform/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/03/29/hello-world/"},{"title":"LLM Tricks","text":"LLM Training TricksMajority of LLM are decoder only with a few tricks for: Data Tricks Architechture Tricks Optimisation tricks Engineering tricks Architechture Tricks LayerNorm after embedding layer ALiBI posiition embedding RoPe position embedding Lazy softmax [ref: self-attention does not need O(n^2 memory)] efficient causal multihead attention (calculate (1/2)n^2 dot products) prenorm residual connections swiGLU Optimisation Tricks AdamW Cosine schedule weight decay gradient clipping dropout optmising backprop to tradeoff between memory and speed. (save more than normal for expensive operations like Wx) Engineering tricksMixed precision trainingUse low precision where possible.fp32, great for accuracy, demanding memory. good for softmaxfp16, less accurate, fast less accuratebfp16 Efficinet cuda ‘stuff’FlashAttention","link":"/2023/06/02/llm_tricks/"}],"tags":[{"name":"optimisation","slug":"optimisation","link":"/tags/optimisation/"},{"name":"deep-learning, computer-vision, self-supervised-learning","slug":"deep-learning-computer-vision-self-supervised-learning","link":"/tags/deep-learning-computer-vision-self-supervised-learning/"},{"name":"signal-processing","slug":"signal-processing","link":"/tags/signal-processing/"},{"name":"deep-learning, llm","slug":"deep-learning-llm","link":"/tags/deep-learning-llm/"}],"categories":[],"pages":[]}