{"posts":[{"title":"Cutting Plane Algorithms","text":"Cutting plane methods are a useful tool for solving optimization problems. In particular they solve non-differentiable convex problems. The general problem they solve is finding any point in a set $X \\in \\mathbb{R}^n $.The cutting-plane method is essentially a generilisatoin of bisectoin in $\\mathbb{R}$. The simplest example is the following: Set constraints: $P_k ={}$ Pick a point $x^{(k)} \\in P_k$ If $x^{(k)} \\in X$ then stop. Calculate a subgradient $g$ at $x$ Add the inequality $g^{T} z \\leq x^{(k)}$ to the set of constraints. goto 2. There are a few options for picking $ $x^{(k)} \\in P_k$. The simplest is to pick a random point in the polyhedron. The others try to pick $x^{(k)}$ in an intelligent way, such that a cut will reduce the search space as much as possible. Four methods are: Center of gravity cutting-plane method Chebyshev center cutting-plane method Maximum volume ellpsoid method Analytic Cutting plane Method Ellipsoid method The extensions all seek to find a better point $x^{(k)}$ than a random selection. We basically want the cutting plane to reduce the search space as much as possible. Therefore the center of the polyhedron is a good choice. Center of Gravity Cutting-plane MethodThis algorithm is so inefficient that it’s considered a theoretical approach. It does highlight exactly what these extensions are trying to do. It picks the next search point as the center of gravity of the polyhedron. $$ x^{(k+1)} = \\dfrac{\\int_{P_k}xdx}{\\int_{P_k}dx} $$ Chebyshev Center Cutting-plane MethodThis method picks the next search point as the center of the largest ball that fits inside the polyhedron. This can be founded via an LP. It a common geometric optimisation problem called Chebshev Centering. $$\\begin{aligned}\\max \\quad &amp; R \\\\textrm{s.t.} \\quad &amp; a_i^T x + R ||a_i||_* \\leq b_i, i = 1,…,m \\\\quad &amp; R \\geq 0 \\\\end{aligned}$$ Maximum volume ellipsoid methodThis is similar to the chebyshev method in that it finds the largest ellipsoid which fits in the polyhedra and then chooses Analytic Cutting Plane MethodThe basic idea here is to find the ‘analytic center’ of the polyhedron. This optimises a log barrier funciton which can be computed using the infeasible start newton method. Ellipsoid MethodThis method is a bit different to the others. It maintains an ellipsoid which represents the current search space. It then finds a cutting plane which cuts the ellipsoid in half. It then shrinks the ellipsoid to minimially cover the polyhedron. This is repeated until the ellipsoid is small enough. Intersting subproblems: Volume of an ellipsoid Minimum cover ellipsoid of polyhedron Center of gravity Maximum volume ellipsoid inside polyhedron Largest ball in polyhedra Questions?Center of ellipsoid?","link":"/2023/04/02/cuttingplane/"},{"title":"Coursera - TensorFlow Advanced Techniques Specialisation","text":"I just finished DeepLearning.AI’s advanced TensorFlow course through coursera. The specialisatoin is divided into four courses; Custom Models, Layers and Loss Functions with TensorFlow Custom and Distributed Training with TensorFlow Advanced Computer Vision with TensorFlow Generative Deep Learning with TensorFlow I’ll go into more detail on each course below, give a few critiques, but I’ll start with a brief overview of my experience with the course. Overall I enjoyed the course but found the second half a bit underwhelming. It brushed me up on some of the finer points of my core knowledge of TensorFlow, but didn’t really teach me anything new. I think the course isn’t as focsued as it should be, it feels like a mixutre of an introudciton to TensorFlow beyond the sequential API and an introductory deep learning course. The course can be found here: TensorFlow: Advanced Techniques Specialisation. Custom Models, Layers and Loss Functions with TensorFlowThis was probably my favourite course in the specialization. It was closer to what I was looking for in terms of the inner working of TensorFlow rather than the applications. The course covered the following topics; Functional vs Sequential API, Custom losses, layers, models and callbacks. Custom and Distributed Training with TensorFlowMost of the distributed training felt supuflous, although some familiaryt with the basic TensorFLow objects for this is useful. The course covered the following topics; Differentiation and gradients with gradient tape, custom training loops, graph mode vs eagerm ode and dsitributed training strategies. Advanced Computer Vision with TensorFlowI didn’t love this course. It was a bit of a whirl-wind tour of using the techniques from course 1 and course 2 to build a few different models accross problems including segmentation, classification and object detection. It was a bit too applied for my taste, I was familiar with most of the concepts and if i’m going to spend time training models I would prefer to do it on my own projects. Generative Deep Learning with TensorFlowI actually quite enjoyed this course despite it’s similarities to course 3. It’s pretty hard not to appreciate the results of the generative techniques we covered. It was broken down into four chunks: neural style transfer, which I had read the paper for but not implemented, autoencoders, variational autoencoders and GANs. Critiques of the SpecializationI have three main criques on the course which I’ll go into in more detail below. Topic choice andgamification of coursera. My problem with the content, primarily in the latter courses, is that it wasn’t very specific to TensorFlow. It was more of a general introduction to deep learning and the techniques used in the field - with implementations in TensorFlow. I think for introductory courses this makes sense, trying to get the earner interested in the field and giving motivating examples. However, I think for a specialization it should be more specific to the framework, really digging into how things work. This would serve me better as a developer. The primary issue i’ve faced with all coursera work is the natural gamification of the platform. The videos, quizes and assignments, quizes are all relatively bite-sized and easy to consume. This is great for getting it done quickly, but it’s not great for retaining information. I’ve found it difficult not to use just get the videos out of the way and smash out the assignment as quickly as possible. I think this is a problem with the platform, not the course. It’s too tempting to just get the work done and move on. I think a more effective approach would be to have longer videos in a more lecture style format, with more content. THe assignments should be removed and replaced with longer quizes, including random sampling of questions to prevent brute forcing, limited retries, and short answer coding questions to really test understanding.This format would create much more pressure on the learner, which is bound to give better results.","link":"/2023/03/29/advanced_tf/"},{"title":"Deep Cluster","text":"IntroDue to the difficult around obtaining good labelled data, self-supervised learning has been increasing in popularity over the last five year. Yann LeCun is a major proponente and his team at Meta AI are constantly publishing papers on the topic. Their recent “Cook Book of Self-Supervised Learning” [arXiv:2304.12210] divides the field into three approaches: self-distillation, constrastive and canonical correlation analysis. Of the three I think that self-distillation is the most surprising and challenges some fundemental intuitions about what works when training these models. They are based on the experimental observation that a linear classifier trainined on the features extracted from a random initalised AlexNet encoder obtains 14% top-1 accuracy on ImageNet, where random guessing is 0.1%. In this blog post I look at one of the early approaches to self-distillation - ‘Deep Cluster’. DeepClusterSelf-distillation models use their own output to generate labels which the model will be retrained on, then the process repeats. Deep Cluster uses an AlexNet encoder as a feature extractor, and applies kmeans to the extracted features to obtain labels. Collapse is a key issue in self-supervised learning. Collapse occurs when the network learns a trival representation to solve the learning task. For example Deep Cluster could collapse by the network learning to output a single embeddding, regardless of input. This would result in each instance being grouped into the same cluster, and then the network can achieve perfect accuracy by predicting that cluster. However a constant embedding isn’t very useful, so the authors have found two mechanisms to prevent collapse. The first mechanism to prevent collapse is to encourage equiparition. Equipartion means that the same number of isntances are grouped into each cluster. To implement this; any cluster that has no instances in it will update it’s centroid to a random other centroid with a small peturbation. This causes it to take about half the other clusters instances. The second mechanism to prevent collapse is to weight the loss contribution of an instance inversely to the size of the cluster. This means large clusters will have less importance per instance, whereas small clusters will have more important instances. Implementation notesI use the FAISS implementation of PCA and kmeans. PCA is applied to the embeddings to reduce them from 9216 to 256. This makes the code more memory efficient. The paper uses whitening but this is too expensive for me to use. I substract the mean, divide by the variance and divide by the L2 norm. Clustering is really slow, it requires a full pass on the dataset.","link":"/2023/05/02/deep_cluster/"},{"title":"Fourier Transform","text":"Fourier transform is one of the most important equations in applied maths. For audio deep learning this transform is used to find the spectrogram - which is a key input to the model. This blog post is a review of the fourier transform. 123import cvxpyimport matplotlib.pyplot as pltimport librosa Begin by loading the wave form and plotting 123456789audio_path = &quot;speech.wav&quot;waveform, sample_rate = librosa.load(audio_path, sr=None) # sr=None preserves sample rateplt.figure(figsize=(14, 5))plt.plot(waveform)plt.title(f&quot;Waveform. sample rate={sample_rate}Hz&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.show() Now lets look at what’s going on zoomed in 12345678910def s(t): return waveform[1000:2000][t]t = np.arange(1000)plt.figure(figsize=(14, 5))plt.plot(s(t))plt.title(f&quot;Waveform. sample rate={sample_rate}Hz&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.show() Okay now a brief review of sinusoids 12345678910111213141516import numpy as npdef sinusoid(t,freq,phase): return np.sin(2 * np.pi * (freq * t - phase))t = np.linspace(0,10,1000)plt.figure(figsize=(14, 5))for i in range(3): y = sinusoid(t, 1, i*0.1) plt.plot(t, y, label=f'phase={i*0.1}')plt.title(f&quot;Sinusoid phase example&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() The phase is a horizontal shift of the sin wave. 12345678910111213141516import numpy as npdef sinusoid(t,freq,phase): return np.sin(2 * np.pi * (freq * t - phase))t = np.linspace(0,10,1000)plt.figure(figsize=(14, 5))for i in range(1,4): y = sinusoid(t, 0.5*i, 1) plt.plot(t, y, label=f'freq={i*0.5}')plt.title(f&quot;Sinusoid frequency example&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() The fourier transform converts from the time domain to the frequency domain. It does this by representing the signal as infinitely many sinusoids of varying frequency. The main steps are: Enumerate accross frequency For each frequency optimise phase to maximise similarity of sinusoid with signal Calculate magnitude as similarity. 12345678910111213freq = 0.013phase = 1plt.figure(figsize=(14, 5))t = np.arange(1000)y = sinusoid(t, freq, phase) * .1plt.plot(s(t), label='origianl signal')plt.plot(t, y, label=f'sinusoid')plt.title(f&quot;Maximising similarity for freq={freq} sinusoid by shifting phase={phase}&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() They’re completly out of phase here. But we can tweak the phase to try have peaks matching. 12345678910111213freq = 0.013phase = 0.5plt.figure(figsize=(14, 5))t = np.arange(1000)y = sinusoid(t, freq, phase) * .1plt.plot(s(t), label='origianl signal')plt.plot(t, y, label=f'sinusoid')plt.title(f&quot;Maximising similarity for freq={freq} sinusoid by shifting phase={phase}&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() That’s a bit better. To measure the similarity between the signal and the sinusoid specified by frequency and phase we use an inner product. $$ \\phi_f = argmax_{\\phi \\in [0,1]} \\left[ \\int s(t) \\sin(2 \\pi (ft - \\phi)) dt\\right] $$ So the similarity is the area of the product of the original signal and the sinusoid. Intuitievly if they’re out of phase they cancel each other and the area is minimised. The magnitude (similarity) is then:$$ \\phi_f = max_{\\phi \\in [0,1]} \\left[ \\int s(t) \\sin(2 \\pi (ft - \\phi)) dt \\right] $$ Looking at the out of phase product: 12345678910111213freq = 0.013phase = 1plt.figure(figsize=(14, 5))t = np.arange(1000)y = sinusoid(t, freq, phase) * .1plt.plot(t, s(t) * y, label=f'product')plt.plot(np.zeros(1000))plt.title(f&quot;Out of phase Product between signal and sinusoid for freq={freq} by shifting phase={phase}&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() Notice it’s mostly negative And now looking at the inphase product: 12345678910111213freq = 0.013phase = 0.5plt.figure(figsize=(14, 5))t = np.arange(1000)y = sinusoid(t, freq, phase) * .1plt.plot(t, s(t) * y, label=f'product')plt.plot(np.zeros(1000))plt.title(f&quot;Product between signal and sinusoid for freq={freq} by shifting phase={phase}&quot;)plt.xlabel(&quot;Samples&quot;)plt.ylabel(&quot;Amplitude&quot;)plt.legend()plt.show() Mostly positive. This in-phase product will have a much higher area, which will be calculated with the integral. So a simple fourier transform would iterative over a bunch of frequencies, for each find phase that maximises inner product with signal, and record the magnitude (area/similarity). Will do that now: 123456789101112131415num_phases = 10num_freqs = 1000freq_max_magnitudes = np.zeros(num_freqs)for freq_i, freq in enumerate(np.linspace(0, 0.1, num_freqs)): phase_magnitudes = np.zeros(num_phases) for phase_i, phase in enumerate(np.linspace(0, 1, num_phases)): magnitude = 0 for t in np.arange(1000): prod = s(t) * np.sin(np.pi * 2 * (freq * t - phase)) magnitude += prod phase_magnitudes[phase_i] = magnitude freq_max_magnitudes[freq_i] = np.max(phase_magnitudes) 12345678plt.figure(figsize=(14, 5))f = np.linspace(0, 0.1, num_freqs)plt.plot(f, freq_max_magnitudes)plt.title(f&quot;Spectrograph&quot;)plt.xlabel(&quot;Frequency&quot;)plt.ylabel(&quot;Magnitude&quot;)plt.legend()plt.show() This concludes the first section on the basics of the fourier transform. I will now move on to representing this in a cleaner way, using complex numbers. Complex Number Fourier TransformThe key idea is encoding the magnitude and phase as a complex number. The magnitude is the length of the complex number, and the phase is the angle. “The complex fourier transform coefficient”. $$\\hat{g}(f) = c_f = \\dfrac{d_f}{\\sqrt{2}} e^{-i2\\pi \\phi f}$$ $$\\hat{g}(f) = $\\int g(t) e^{-i 2 \\pi f t}dt$Where $c_f$ is the complex fourier transform coefficient. THe fourier transform is the gunction $\\hat{g} : R \\rightarrow C$, a function mapping forom real numbers (frequency) to complex numbers (complex fourier transform coefficient). 12345678910import matplotlib.pyplot as pltimport numpy as npfrom PIL import Imageimport imageiofrom IPython.display import Image, display# Generate your matplotlib plots# Replace the code below with your own plot generation logic First a remineder on complex numbers . $ i = \\sqrt{ -1 }$ Usually expressed with real and complext part z = 3 + 2j","link":"/2023/05/15/fourier_transform/"},{"title":"LLM Tricks","text":"Majority of LLM are decoder only with a few tricks for: Data Tricks Architechture Tricks Optimisation tricks Engineering tricks Architechture Tricks LayerNorm after embedding layer ALiBI posiition embedding RoPe position embedding Lazy softmax [ref: self-attention does not need O(n^2 memory)] efficient causal multihead attention (calculate (1/2)n^2 dot products) prenorm residual connections swiGLU Optimisation Tricks AdamW Cosine schedule weight decay gradient clipping dropout optmising backprop to tradeoff between memory and speed. (save more than normal for expensive operations like Wx) Engineering tricksMixed precision trainingUse low precision where possible.fp32, great for accuracy, demanding memory. good for softmaxfp16, less accurate, fast less accuratebfp16 Efficinet cuda ‘stuff’FlashAttention","link":"/2023/06/02/llm_tricks/"}],"tags":[{"name":"optimisation","slug":"optimisation","link":"/tags/optimisation/"},{"name":"tensorflow","slug":"tensorflow","link":"/tags/tensorflow/"},{"name":"deep-learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"computer-vision","slug":"computer-vision","link":"/tags/computer-vision/"},{"name":"self-supervised-learning","slug":"self-supervised-learning","link":"/tags/self-supervised-learning/"},{"name":"signal-processing","slug":"signal-processing","link":"/tags/signal-processing/"},{"name":"llm","slug":"llm","link":"/tags/llm/"}],"categories":[],"pages":[{"title":"about","text":"I’m a Machine Learning Engineer from New Zealand with a computer science background focused on Artificial Intelligence and Machine Learning at Victoria University of Wellington. Previously, I worked at an AI startup as a data scientist and machine learning engineer, utilizing deep computer vision to solve remote sensing segmentation problems.","link":"/About/index.html"}]}